import cv2
import mediapipe as mp
import numpy as np
import joblib
from PIL import ImageFont, ImageDraw, Image

# === í•œê¸€ ì¶œë ¥ í•¨ìˆ˜ ===
def draw_korean_text(img, text, position=(30, 50), font_size=40, color=(0, 200, 0)):
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    font_path = "C:/Windows/Fonts/H2GTRE.TTF"  # HYê²¬ê³ ë”• ê²½ë¡œ
    font = ImageFont.truetype(font_path, font_size)
    draw.text(position, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# === ì† ëœë“œë§ˆí¬ë¥¼ ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜ ===
def get_relative_landmarks(hand_landmarks):
    base = hand_landmarks[0]
    relative = []
    for lm in hand_landmarks:
        relative.extend([lm.x - base.x, lm.y - base.y, lm.z - base.z])
    return relative

# === ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ë¼ë²¨ ì¸ì½”ë” ë¡œë”© ===
clf = joblib.load("models/sign_mlp_model.pkl")
scaler = joblib.load("models/sign_scaler.pkl")
label_encoder = joblib.load("models/sign_label_encoder.pkl")

# === MediaPipe ì´ˆê¸°í™” ===
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2)
mp_drawing = mp.solutions.drawing_utils

# === ì›¹ìº  ì‹œì‘ ===
cap = cv2.VideoCapture(0)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = hands.process(rgb)
    label_text = ""

    if result.multi_hand_landmarks:
        features = []

        # ë‘ ì† ì¢Œí‘œ ëª¨ë‘ ì¶”ì¶œ
        for hand_landmarks in result.multi_hand_landmarks:
            coords = get_relative_landmarks(hand_landmarks.landmark)
            if len(coords) == 63:
                features.append(coords)

        # ì–‘ì† ë‹¤ ìˆì„ ë•Œë§Œ ì˜ˆì¸¡ ì§„í–‰
        if len(features) == 2:
            full_feature = features[0] + features[1]  # ì´ 126ì°¨ì›
            X = np.array(full_feature).reshape(1, -1)
            X_scaled = scaler.transform(X)

            probs = clf.predict_proba(X_scaled)[0]
            confidence = np.max(probs)

            if confidence < 0.7:
                label_text = "ëª¨ë¥´ê² ìŒ"
            else:
                pred = clf.predict(X_scaled)
                label_text = label_encoder.inverse_transform(pred)[0]

            # í•œê¸€ ì¶œë ¥
            frame = draw_korean_text(frame, f"ğŸ‘‰ {label_text}", position=(30, 50))

        # ëœë“œë§ˆí¬ ì‹œê°í™”
        for hand_landmarks in result.multi_hand_landmarks:
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

    cv2.imshow("Sign Language Recognition", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
